{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is inspired by the work of CEM (Zarlenga et al. 2022) and CBM (Koh et al. 2020) papers. Please visit their GitHub repositories:\n",
    "[CEM GitHub](https://github.com/mateoespinosa/cem) and [CBM GitHub](https://github.com/yewsiang/ConceptBottleneck).\n",
    "\n",
    "# CBM: oracle metrics - non-confounded data\n",
    "\n",
    "There are four main steps:\n",
    "1. Loading the dataset.\n",
    "2. Initializing a CBM with InceptionV3 vision backbone for the dataset.\n",
    "3. Load CBMs\n",
    "4. Evaluate the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "The first step is to load the data. The designed CBM class with the PyTorch Lightning Trainer takes in PyTorch DataLoader object.\n",
    "Furthermore, it needs to contain three elements (in the following order):\n",
    "1. the sample image, $\\mathbf{x}$\n",
    "2. the image label, $\\mathbf{y}$\n",
    "3. the concept labels, in binary format, $\\mathbf{c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:13:53.502756Z",
     "iopub.status.busy": "2025-02-10T15:13:53.502355Z",
     "iopub.status.idle": "2025-02-10T15:13:53.506824Z",
     "shell.execute_reply": "2025-02-10T15:13:53.505857Z",
     "shell.execute_reply.started": "2025-02-10T15:13:53.502719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from cub_data_module import *\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:13:54.839124Z",
     "iopub.status.busy": "2025-02-10T15:13:54.838752Z",
     "iopub.status.idle": "2025-02-10T15:14:04.508607Z",
     "shell.execute_reply": "2025-02-10T15:14:04.507729Z",
     "shell.execute_reply.started": "2025-02-10T15:13:54.839094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import sys #for Kaggle\n",
    "#sys.path.append(\"/kaggle/usr/lib/cub_data_module_confounded\") #for Kaggle\n",
    "\n",
    "import src.cub_data_module_no_shuffle as cub_data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:12.718027Z",
     "iopub.status.busy": "2025-02-10T15:14:12.717546Z",
     "iopub.status.idle": "2025-02-10T15:14:12.725664Z",
     "shell.execute_reply": "2025-02-10T15:14:12.724780Z",
     "shell.execute_reply.started": "2025-02-10T15:14:12.718000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _update_config_with_dataset(\n",
    "    config,\n",
    "    train_dl,\n",
    "    n_concepts,\n",
    "    n_tasks,\n",
    "    concept_map,\n",
    "):\n",
    "    config[\"n_concepts\"] = config.get(\n",
    "        \"n_concepts\",\n",
    "        n_concepts,\n",
    "    )\n",
    "    config[\"n_tasks\"] = config.get(\n",
    "        \"n_tasks\",\n",
    "        n_tasks,\n",
    "    )\n",
    "    config[\"concept_map\"] = config.get(\n",
    "        \"concept_map\",\n",
    "        concept_map,\n",
    "    )\n",
    "\n",
    "    task_class_weights = None\n",
    "\n",
    "    if config.get('use_task_class_weights', False):\n",
    "        logging.info(\n",
    "            f\"Computing task class weights in the training dataset with \"\n",
    "            f\"size {len(train_dl)}...\"\n",
    "        )\n",
    "        attribute_count = np.zeros((max(n_tasks, 2),))\n",
    "        samples_seen = 0\n",
    "        for i, data in enumerate(train_dl):\n",
    "            if len(data) == 2:\n",
    "                (_, (y, _)) = data\n",
    "            else:\n",
    "                (_, y, _) = data\n",
    "            if n_tasks > 1:\n",
    "                y = torch.nn.functional.one_hot(\n",
    "                    y,\n",
    "                    num_classes=n_tasks,\n",
    "                ).cpu().detach().numpy()\n",
    "            else:\n",
    "                y = torch.cat(\n",
    "                    [torch.unsqueeze(1 - y, dim=-1), torch.unsqueeze(y, dim=-1)],\n",
    "                    dim=-1,\n",
    "                ).cpu().detach().numpy()\n",
    "            attribute_count += np.sum(y, axis=0)\n",
    "            samples_seen += y.shape[0]\n",
    "        print(\"Class distribution is:\", attribute_count / samples_seen)\n",
    "        if n_tasks > 1:\n",
    "            task_class_weights = samples_seen / attribute_count - 1\n",
    "        else:\n",
    "            task_class_weights = np.array(\n",
    "                [attribute_count[0]/attribute_count[1]]\n",
    "            )\n",
    "    return task_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:12.727003Z",
     "iopub.status.busy": "2025-02-10T15:14:12.726713Z",
     "iopub.status.idle": "2025-02-10T15:14:12.742278Z",
     "shell.execute_reply": "2025-02-10T15:14:12.741467Z",
     "shell.execute_reply.started": "2025-02-10T15:14:12.726976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _generate_dataset_and_update_config(\n",
    "    experiment_config\n",
    "):\n",
    "    if experiment_config.get(\"dataset_config\", None) is None:\n",
    "        raise ValueError(\n",
    "            \"A dataset_config must be provided for each experiment run!\"\n",
    "        )\n",
    "\n",
    "    dataset_config = experiment_config['dataset_config']\n",
    "    logging.debug(\n",
    "        f\"The dataset's root directory is {dataset_config.get('root_dir')}\"\n",
    "    )\n",
    "    intervention_config = experiment_config.get('intervention_config', {})\n",
    "    if dataset_config[\"dataset\"] == \"cub\":\n",
    "        data_module = cub_data_module\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset {dataset_config['dataset']}!\")\n",
    "\n",
    "    train_dl, val_dl, test_dl, imbalance, (n_concepts, n_tasks, concept_map) = \\\n",
    "        data_module.generate_data(\n",
    "            config=dataset_config,\n",
    "            seed=42,\n",
    "            output_dataset_vars=True,\n",
    "            root_dir=dataset_config.get('root_dir', None),\n",
    "            model_inspection=False,\n",
    "        )\n",
    "    # For now, we assume that all concepts have the same\n",
    "    # aquisition cost\n",
    "    acquisition_costs = None\n",
    "    if concept_map is not None:\n",
    "        intervened_groups = list(\n",
    "            range(\n",
    "                0,\n",
    "                len(concept_map) + 1,\n",
    "                intervention_config.get('intervention_freq', 1),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        intervened_groups = list(\n",
    "            range(\n",
    "                0,\n",
    "                n_concepts + 1,\n",
    "                intervention_config.get('intervention_freq', 1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    task_class_weights = _update_config_with_dataset(\n",
    "        config=experiment_config,\n",
    "        train_dl=train_dl,\n",
    "        n_concepts=n_concepts,\n",
    "        n_tasks=n_tasks,\n",
    "        concept_map=concept_map,\n",
    "    )\n",
    "    return (\n",
    "        train_dl,\n",
    "        val_dl,\n",
    "        test_dl,\n",
    "        imbalance,\n",
    "        concept_map,\n",
    "        intervened_groups,\n",
    "        task_class_weights,\n",
    "        acquisition_costs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:12.744023Z",
     "iopub.status.busy": "2025-02-10T15:14:12.743779Z",
     "iopub.status.idle": "2025-02-10T15:14:12.779481Z",
     "shell.execute_reply": "2025-02-10T15:14:12.778645Z",
     "shell.execute_reply.started": "2025-02-10T15:14:12.744004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yaml_path = \"data/cub.yaml\" # for local development, might need to use whole path.\n",
    "\n",
    "with open(yaml_path, \"r\") as file:\n",
    "    yaml_config = yaml.safe_load(file)\n",
    "yaml_config[\"shared_params\"][\"dataset_config\"][\"root_dir\"] = \"/kaggle/input/cem-cub2000-filtered/\" #for Kaggle, replace this with locally downloaded folder.\n",
    "yaml_config[\"shared_params\"][\"dataset_config\"][\"num_workers\"] = 4 #change depending on resources available.\n",
    "yaml_config[\"shared_params\"][\"dataset_config\"][\"batch_size\"] = 64 #change depending on resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:12.781117Z",
     "iopub.status.busy": "2025-02-10T15:14:12.780805Z",
     "iopub.status.idle": "2025-02-10T15:14:14.123490Z",
     "shell.execute_reply": "2025-02-10T15:14:14.122568Z",
     "shell.execute_reply.started": "2025-02-10T15:14:12.781084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl, imbalance, concept_map, intervened_groups, task_class_weights, acquisition_costs = _generate_dataset_and_update_config(yaml_config[\"shared_params\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the CBM\n",
    "### Step 2.1 Define model for input to concepts\n",
    "We first need to define a architecture that will extract the concepts from the input image.\n",
    "\n",
    "For this, we used a pre-trained InceptionV3 model. We remove the last linear layer and make one that we can use for our task, so it is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:14.124673Z",
     "iopub.status.busy": "2025-02-10T15:14:14.124370Z",
     "iopub.status.idle": "2025-02-10T15:14:14.129017Z",
     "shell.execute_reply": "2025-02-10T15:14:14.128048Z",
     "shell.execute_reply.started": "2025-02-10T15:14:14.124639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def latent_code_generator_model(output_dim=112):\n",
    "    # Load pre-trained InceptionV3\n",
    "    inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "\n",
    "    # Remove auxiliary classifier (set to None)\n",
    "    inception.aux_logits = False  # Disable aux_logits\n",
    "    inception.AuxLogits = None  # Delete aux classifier branch\n",
    "\n",
    "    inception.fc = torch.nn.Linear(2048, output_dim)  # Replace classification layer with output_dim\n",
    "\n",
    "    return inception"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: define CBM model.\n",
    "We need to define the following:\n",
    "1. `n_concepts`: the number of concepts in the dataset (112).\n",
    "2. `n_tasks`: the number of output labels in the dataset (200).\n",
    "3. `concept_loss_weight`: the weight to use for the concept prediction loss during training of the CBM. Picked to be the same as the CEM paper.\n",
    "4. `learning_rate` and `optimizer`: to use during training. Optimizer is Adam by default, otherwise SGD.\n",
    "5. `c_extractor_arch`: the model architecture to use for going from the input space to the concepts.\n",
    "6. `c2y_model` and `c2y_layers`: the model architecture to use for going from the concepts to the labels. It can be directly the model, like c_extractor_arch or the layers as a list. We choose to do a simple linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:14.130084Z",
     "iopub.status.busy": "2025-02-10T15:14:14.129792Z",
     "iopub.status.idle": "2025-02-10T15:14:14.151697Z",
     "shell.execute_reply": "2025-02-10T15:14:14.150937Z",
     "shell.execute_reply.started": "2025-02-10T15:14:14.130058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.utils_cbm import *\n",
    "from src.cbm import ConceptBottleneckModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the CBMs\n",
    "\n",
    "Now that we have both the dataset and the model defined, we can train our CEM\n",
    "using Pytorch Lightning's wrappers for ease. This should be very simple via\n",
    "Pytorch Lightning's `Trainer` once the data has been generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:14.260709Z",
     "iopub.status.busy": "2025-02-10T15:14:14.260455Z",
     "iopub.status.idle": "2025-02-10T15:14:14.277431Z",
     "shell.execute_reply": "2025-02-10T15:14:14.276687Z",
     "shell.execute_reply.started": "2025-02-10T15:14:14.260681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(typ):\n",
    "    cbm_model_new = ConceptBottleneckModel.load_from_checkpoint(\n",
    "        checkpoint_path=f\"/kaggle/input/oracles-v2/{typ}.ckpt\",\n",
    "        n_concepts=112,\n",
    "        n_tasks=200,\n",
    "        concept_loss_weight=yaml_config[\"shared_params\"][\"concept_loss_weight\"],\n",
    "        learning_rate=yaml_config[\"shared_params\"][\"learning_rate\"],  # The learning rate to use during training.\n",
    "        optimizer=\"sgd\",\n",
    "        c_extractor_arch=latent_code_generator_model, # Here we provide our generating function for the latent code generator model.\n",
    "        c2y_model=None,\n",
    "    )\n",
    "    \n",
    "    return cbm_model_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: evaluation\n",
    "Now, we evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:14:14.278310Z",
     "iopub.status.busy": "2025-02-10T15:14:14.278124Z",
     "iopub.status.idle": "2025-02-10T15:15:17.103972Z",
     "shell.execute_reply": "2025-02-10T15:15:17.103136Z",
     "shell.execute_reply.started": "2025-02-10T15:14:14.278294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Before anything, however, let's get the underlying numpy arrays of our\n",
    "# dataset as they will be easier to work with\n",
    "x_test, y_test, c_test = [], [], []\n",
    "for (x, y, c) in tqdm(test_dl):\n",
    "    x_test.append(x)\n",
    "    y_test.append(y)\n",
    "    c_test.append(c)\n",
    "x_test = np.concatenate(x_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "c_test = np.concatenate(c_test, axis=0)\n",
    "\n",
    "x_train, y_train, c_train = [], [], []\n",
    "for (x, y, c) in tqdm(train_dl):\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "    c_train.append(c)\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "c_train = np.concatenate(c_train, axis=0)\n",
    "\n",
    "x_val, y_val, c_val = [], [], []\n",
    "for (x, y, c) in tqdm(val_dl):\n",
    "    x_val.append(x)\n",
    "    y_val.append(y)\n",
    "    c_val.append(c)\n",
    "x_val = np.concatenate(x_val, axis=0)\n",
    "y_val = np.concatenate(y_val, axis=0)\n",
    "c_val = np.concatenate(c_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:15:17.106312Z",
     "iopub.status.busy": "2025-02-10T15:15:17.105877Z",
     "iopub.status.idle": "2025-02-10T15:15:17.114045Z",
     "shell.execute_reply": "2025-02-10T15:15:17.113176Z",
     "shell.execute_reply.started": "2025-02-10T15:15:17.106277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, cbm, x, y, c):\n",
    "    #Now we are ready to generate the concept, label, and embedding predictions for\n",
    "    #the test set using our trained CEM:\n",
    "\n",
    "    # We will use a Trainer object to run inference in batches over our test\n",
    "    # dataset\n",
    "    trainer_inference = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=\"auto\",\n",
    "        logger=False, # No logs to be dumped for this trainer\n",
    "    )\n",
    "    batch_results = trainer_inference.predict(cbm, dataloader)\n",
    "    \n",
    "    # Then we combine all results into numpy arrays by joining over the batch\n",
    "    # dimension\n",
    "    c_pred = np.concatenate(\n",
    "        list(map(lambda x: x[0].detach().cpu().numpy(), batch_results)),\n",
    "        axis=0,\n",
    "    )\n",
    "    c_embs = np.concatenate(\n",
    "        list(map(lambda x: x[1].detach().cpu().numpy(), batch_results)),\n",
    "        axis=0,\n",
    "    )\n",
    "    # Reshape them so that we have embeddings (batch_size, k, emb_size)\n",
    "    c_embs = np.reshape(c_embs, (c.shape[0], c.shape[1], -1))\n",
    "    \n",
    "    y_pred = np.concatenate(\n",
    "        list(map(lambda x: x[2].detach().cpu().numpy(), batch_results)),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    ##And compute all the metrics of interest:\n",
    "    # To match the dimensions of y_pred (5794, 200), and y (5794,):\n",
    "    # We need to apply a softmax layer to get the class probabilities\n",
    "    y_prob = softmax(y_pred, axis=1)\n",
    "    \n",
    "    # Then we get the highest probability for the classes\n",
    "    y_pred_classes = np.argmax(y_prob, axis=1)  # Shape (5794,)\n",
    "\n",
    "\n",
    "\n",
    "    ##########\n",
    "    ## Compute test task accuracy\n",
    "    ##########\n",
    "    task_accuracy = accuracy_score(y, y_pred_classes)\n",
    "    print(f\"Our CBM's test task accuracy is {task_accuracy*100:.2f}%\")\n",
    "\n",
    "    ##########\n",
    "    ## Compute test concept AUC\n",
    "    ##########\n",
    "    concept_auc = roc_auc_score(c, c_pred)\n",
    "    print(f\"Our CBM's test concept AUC is {concept_auc*100:.2f}%\")\n",
    "\n",
    "    return y_pred_classes, c_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:15:17.115181Z",
     "iopub.status.busy": "2025-02-10T15:15:17.114898Z",
     "iopub.status.idle": "2025-02-10T15:33:54.158978Z",
     "shell.execute_reply": "2025-02-10T15:33:54.158129Z",
     "shell.execute_reply.started": "2025-02-10T15:15:17.115158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#for i in range(10, 109, 10):\n",
    "l = [10, 20, 30, 50, 100]\n",
    "for i in l:\n",
    "    typ = f\"cmb_{i}\"\n",
    "    cbm = load_model(typ)\n",
    "    print(f\"Model with epochs {i}\")\n",
    "    print(\"TRAIN\")\n",
    "    y_pred_classes, c_pred = evaluate_model(train_dl, cbm, x_train, y_train, c_train)\n",
    "    print(\"VAL\")\n",
    "    y_pred_classes, c_pred = evaluate_model(val_dl, cbm, x_val, y_val, c_val)\n",
    "    print(\"TEST\")\n",
    "    y_pred_classes, c_pred = evaluate_model(test_dl, cbm, x_test, y_test, c_test)\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2534241,
     "sourceId": 5140550,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6600121,
     "sourceId": 10658340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6630020,
     "sourceId": 10698817,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6640507,
     "sourceId": 10713535,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 220467553,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 220470336,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221554220,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
